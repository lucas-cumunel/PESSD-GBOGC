{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84633202-8c0f-4689-a132-286bdbd1580a",
   "metadata": {},
   "source": [
    "On installe les packages nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04c3ba38-b60a-4f3e-af5f-b5e98b4ba50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vosk in /opt/python/lib/python3.13/site-packages (0.3.45)\n",
      "Requirement already satisfied: soundfile in /opt/python/lib/python3.13/site-packages (0.13.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/python/lib/python3.13/site-packages (from vosk) (2.0.0)\n",
      "Requirement already satisfied: requests in /opt/python/lib/python3.13/site-packages (from vosk) (2.32.5)\n",
      "Requirement already satisfied: tqdm in /opt/python/lib/python3.13/site-packages (from vosk) (4.67.1)\n",
      "Requirement already satisfied: srt in /opt/python/lib/python3.13/site-packages (from vosk) (3.5.3)\n",
      "Requirement already satisfied: websockets in /opt/python/lib/python3.13/site-packages (from vosk) (15.0.1)\n",
      "Requirement already satisfied: numpy in /opt/python/lib/python3.13/site-packages (from soundfile) (2.3.5)\n",
      "Requirement already satisfied: pycparser in /opt/python/lib/python3.13/site-packages (from cffi>=1.0->vosk) (2.23)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests->vosk) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests->vosk) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests->vosk) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/lib/python3.13/site-packages (from requests->vosk) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vosk soundfile\n",
    "print(\"installation faite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60bf8c1e-9449-4bf4-b855-033fced76275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importation faite\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import wave\n",
    "import json\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import soundfile as sf\n",
    "print(\"importation faite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a937ba9-ff2c-4763-b134-7ae8b18fe5fe",
   "metadata": {},
   "source": [
    "On transforme notre fichier mp3 en fichier wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4bbf782-04b8-425c-9e24-cc5df6a97475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion MP3 → WAV terminée.\n"
     ]
    }
   ],
   "source": [
    "input_file = \"videoplayback-_1_.wav\"\n",
    "output_wav = \"audio_temp.wav\"\n",
    "\n",
    "data, samplerate = sf.read(input_file)\n",
    "sf.write(output_wav, data, samplerate, subtype='PCM_16')\n",
    "\n",
    "print(\"Conversion MP3 → WAV terminée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c11b0237-6dda-42ed-b282-d1684f8b07e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=4\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from vosk-model-small-fr-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from vosk-model-small-fr-0.22/graph/HCLr.fst vosk-model-small-fr-0.22/graph/Gr.fst\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle chargé\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo vosk-model-small-fr-0.22/graph/phones/word_boundary.int\n"
     ]
    }
   ],
   "source": [
    "# On charge notre modèle\n",
    "model = Model(\"vosk-model-small-fr-0.22\")\n",
    "print(\"Modèle chargé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab43ebb-c2b1-438c-934c-c60bd16788e7",
   "metadata": {},
   "source": [
    "Maintenant que notre modèle est chargé et notre fichier mp3 convertit en wav, on ouvre le fichier audio :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12b2235d-ef7a-4b17-9ddb-378ce4a48c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle est prêt à recevoir l'audio\n"
     ]
    }
   ],
   "source": [
    "# ouverture du fichier audio\n",
    "wf = wave.open(output_wav, \"rb\")\n",
    "\n",
    "#initialisation du modèle Vosk\n",
    "rec = KaldiRecognizer(model, wf.getframerate())\n",
    "\n",
    "print(\"Le modèle est prêt à recevoir l'audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c50df7-7300-4fd7-8aa4-f8d5b170a1e0",
   "metadata": {},
   "source": [
    "On fait la retranscription de l'audio en texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "694647b8-0578-4122-9887-5e2f4cce3404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de la transcription...\n",
      "Transcription terminée !\n",
      "\n",
      "un peu après\n",
      "je viens\n",
      "chien cherche a eu mon chien\n",
      "tiens\n",
      "j'ai\n",
      "et un ans\n",
      "michael schumacher chic\n",
      "mon bébé faveur\n",
      "un châtiment\n",
      "vente de\n",
      "chaque\n",
      "les branches\n",
      "tu ne peux pas\n",
      "elle\n",
      "chat\n",
      "en bref un ranch\n",
      "je prendrai\n",
      "allez un peu sombre\n",
      "cher\n",
      "très bon\n",
      "chance\n",
      "parents champêtre chic\n",
      "un vrai\n",
      "chance\n",
      "mon la l'infini\n",
      "ton patch\n",
      "perche\n",
      "c'est sûr\n",
      "chef de chantier\n",
      "la pêche\n",
      "tu pas\n",
      "hum\n",
      "merci\n",
      "\n",
      "chaque\n",
      "\n",
      "cette maladie\n",
      "fait\n",
      "a eu\n",
      "perche méchant\n",
      "pas cher\n",
      "c'est ainsi\n",
      "chine\n",
      "mon\n",
      "vous\n",
      "perche\n",
      "deux\n",
      "bon\n",
      "chanteur\n",
      "un coup\n",
      "qui\n",
      "mon père de un peu café\n",
      "rachète une chanson\n",
      "un peu\n",
      "chat\n",
      "prend en charge\n",
      "c'est bête\n",
      "mon chien\n",
      "tu\n",
      "j'aimerais\n",
      "tu\n",
      "en fait\n",
      "h\n",
      "manger\n",
      "h\n",
      "oui\n",
      "chanson impôts\n",
      "de\n",
      "tu choisirais\n",
      "\n",
      "\n",
      "dimanche\n",
      "chirurgien\n",
      "cinq\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# création du fichier qui acceuil le texete de la transcription\n",
    "transcript = \"\"\n",
    "print(\"Début de la transcription...\")\n",
    "\n",
    "# début de la transcription\n",
    "while True:\n",
    "    data = wf.readframes(4000)  \n",
    "    if len(data) == 0:\n",
    "        break\n",
    "\n",
    "    if rec.AcceptWaveform(data):\n",
    "        res = json.loads(rec.Result())\n",
    "        transcript += res.get(\"text\", \"\") + \"\\n\"\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Résultat final\n",
    "final_res = json.loads(rec.FinalResult())\n",
    "transcript += final_res.get(\"text\", \"\")\n",
    "\n",
    "print(\"Transcription terminée !\")\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46645e76-8930-462d-b5b4-38feec6a5ff7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Afficher et sauvegarder\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtext\u001b[49m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtranscription.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m     f.write(text)\n",
      "\u001b[31mNameError\u001b[39m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# Afficher et sauvegarder\n",
    "print(text)\n",
    "with open(\"transcription.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "print(\"C'est fini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d35da9c0-9d13-49ae-9584-c30224f0ea16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je suis content\n"
     ]
    }
   ],
   "source": [
    "print(\"je suis content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b913f6-9c92-47cf-821f-ca2cd6cb456b",
   "metadata": {},
   "source": [
    "On passe à Whisper :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76034cc7-752c-4ca2-8043-4bca190e7b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in /opt/python/lib/python3.13/site-packages (20250625)\n",
      "Requirement already satisfied: more-itertools in /opt/python/lib/python3.13/site-packages (from openai-whisper) (10.8.0)\n",
      "Requirement already satisfied: numba in /opt/python/lib/python3.13/site-packages (from openai-whisper) (0.62.1)\n",
      "Requirement already satisfied: numpy in /opt/python/lib/python3.13/site-packages (from openai-whisper) (2.3.5)\n",
      "Requirement already satisfied: tiktoken in /opt/python/lib/python3.13/site-packages (from openai-whisper) (0.12.0)\n",
      "Requirement already satisfied: torch in /opt/python/lib/python3.13/site-packages (from openai-whisper) (2.8.0)\n",
      "Requirement already satisfied: tqdm in /opt/python/lib/python3.13/site-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: triton>=2 in /opt/python/lib/python3.13/site-packages (from openai-whisper) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/python/lib/python3.13/site-packages (from triton>=2->openai-whisper) (80.9.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /opt/python/lib/python3.13/site-packages (from numba->openai-whisper) (0.45.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/python/lib/python3.13/site-packages (from tiktoken->openai-whisper) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/python/lib/python3.13/site-packages (from tiktoken->openai-whisper) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.11.12)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (3.6)\n",
      "Requirement already satisfied: jinja2 in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/python/lib/python3.13/site-packages (from torch->openai-whisper) (1.13.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/python/lib/python3.13/site-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/python/lib/python3.13/site-packages (from jinja2->torch->openai-whisper) (3.0.3)\n",
      "installation terminée\n"
     ]
    }
   ],
   "source": [
    "# 1) Installation de Whisper\n",
    "!pip install -U openai-whisper\n",
    "import time\n",
    "print(\"installation terminée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78766034-7b87-4b31-99f8-6c1d270f58aa",
   "metadata": {},
   "source": [
    "Maintenant que whisper est installé, on importe le modèle choisi parmi 'small', 'medium' ou 'large'. On commence par small --> le moins puissant mais le plus rapide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84451c24-65bf-4b76-9202-e66d4bd7ba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modèle initialisé\n"
     ]
    }
   ],
   "source": [
    "# 2) Importer et charger le modèle\n",
    "import whisper\n",
    "model = whisper.load_model(\"large\") \n",
    "print(\"modèle initialisé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a1d1e7-f211-476a-98dd-4e9ad1e174d4",
   "metadata": {},
   "source": [
    "On installe ffmpeg pour pouvoir lire les fichiers audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5878e141-6e3d-489b-a281-7c9b55f53c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg-downloader in /opt/python/lib/python3.13/site-packages (0.4.1)\n",
      "Requirement already satisfied: requests in /opt/python/lib/python3.13/site-packages (from ffmpeg-downloader) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.40.0 in /opt/python/lib/python3.13/site-packages (from ffmpeg-downloader) (4.67.1)\n",
      "Requirement already satisfied: platformdirs in /opt/python/lib/python3.13/site-packages (from ffmpeg-downloader) (4.5.0)\n",
      "Requirement already satisfied: packaging in /opt/python/lib/python3.13/site-packages (from ffmpeg-downloader) (25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests->ffmpeg-downloader) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests->ffmpeg-downloader) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests->ffmpeg-downloader) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/lib/python3.13/site-packages (from requests->ffmpeg-downloader) (2025.11.12)\n",
      "installateur de ffmpeg téléchargé\n"
     ]
    }
   ],
   "source": [
    "!pip install ffmpeg-downloader\n",
    "print(\"installateur de ffmpeg téléchargé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9476332-a6f7-4d1a-a29a-f4b8524b22f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg-binaries in /opt/python/lib/python3.13/site-packages (1.0.1)\n",
      "Requirement already satisfied: requests in /opt/python/lib/python3.13/site-packages (from ffmpeg-binaries) (2.32.5)\n",
      "Requirement already satisfied: patool in /opt/python/lib/python3.13/site-packages (from ffmpeg-binaries) (4.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests->ffmpeg-binaries) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests->ffmpeg-binaries) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests->ffmpeg-binaries) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/lib/python3.13/site-packages (from requests->ffmpeg-binaries) (2025.11.12)\n",
      "tâche 1 terminée\n",
      "tâche 2 terminée\n",
      "ffmpeg version 6.0-static https://johnvansickle.com/ffmpeg/  Copyright (c) 2000-2023 the FFmpeg developers\n",
      "built with gcc 8 (Debian 8.3.0-6)\n",
      "configuration: --enable-gpl --enable-version3 --enable-static --disable-debug --disable-ffplay --disable-indev=sndio --disable-outdev=sndio --cc=gcc --enable-fontconfig --enable-frei0r --enable-gnutls --enable-gmp --enable-libgme --enable-gray --enable-libaom --enable-libfribidi --enable-libass --enable-libvmaf --enable-libfreetype --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-librubberband --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libvorbis --enable-libopus --enable-libtheora --enable-libvidstab --enable-libvo-amrwbenc --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libdav1d --enable-libxvid --enable-libzvbi --enable-libzimg\n",
      "libavutil      58.  2.100 / 58.  2.100\n",
      "libavcodec     60.  3.100 / 60.  3.100\n",
      "libavformat    60.  3.100 / 60.  3.100\n",
      "libavdevice    60.  1.100 / 60.  1.100\n",
      "libavfilter     9.  3.100 /  9.  3.100\n",
      "libswscale      7.  1.100 /  7.  1.100\n",
      "libswresample   4. 10.100 /  4. 10.100\n",
      "libpostproc    57.  1.100 / 57.  1.100\n",
      "check final\n"
     ]
    }
   ],
   "source": [
    "!pip install ffmpeg-binaries\n",
    "print(\"tâche 1 terminée\")\n",
    "import ffmpeg \n",
    "ffmpeg.init()      # ou ffmpeg.add_to_path()\n",
    "print(\"tâche 2 terminée\")\n",
    "!ffmpeg -version\n",
    "print(\"check final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1506aead-39e3-4117-8a76-4a10ccc1b9ba",
   "metadata": {},
   "source": [
    "On fait la transcription du fichier audio .wav en texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b94e56-f07e-4cbe-93b4-e4241abb9fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 3) Transcription du fichier\u001b[39;00m\n\u001b[32m      3\u001b[39m start = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfinal_jo_eurosport.mp3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m      5\u001b[39m end = time.time()\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtranscription terminée\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/whisper/transcribe.py:295\u001b[39m, in \u001b[36mtranscribe\u001b[39m\u001b[34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, carry_initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    293\u001b[39m     decode_options[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m] = all_tokens[prompt_reset_since:]\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m result: DecodingResult = \u001b[43mdecode_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m tokens = torch.tensor(result.tokens)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m no_speech_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    299\u001b[39m     \u001b[38;5;66;03m# no voice activity check\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/whisper/transcribe.py:201\u001b[39m, in \u001b[36mtranscribe.<locals>.decode_with_fallback\u001b[39m\u001b[34m(segment)\u001b[39m\n\u001b[32m    198\u001b[39m     kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mbest_of\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    200\u001b[39m options = DecodingOptions(**kwargs, temperature=t)\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m decode_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m needs_fallback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    205\u001b[39m     compression_ratio_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    206\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m decode_result.compression_ratio > compression_ratio_threshold\n\u001b[32m    207\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/whisper/decoding.py:824\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(model, mel, options, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[32m    822\u001b[39m     options = replace(options, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m result = \u001b[43mDecodingTask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m single \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/whisper/decoding.py:737\u001b[39m, in \u001b[36mDecodingTask.run\u001b[39m\u001b[34m(self, mel)\u001b[39m\n\u001b[32m    734\u001b[39m tokens = tokens.repeat_interleave(\u001b[38;5;28mself\u001b[39m.n_group, dim=\u001b[32m0\u001b[39m).to(audio_features.device)\n\u001b[32m    736\u001b[39m \u001b[38;5;66;03m# call the main sampling loop\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m tokens, sum_logprobs, no_speech_probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_main_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[38;5;66;03m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001b[39;00m\n\u001b[32m    740\u001b[39m audio_features = audio_features[:: \u001b[38;5;28mself\u001b[39m.n_group]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/whisper/decoding.py:687\u001b[39m, in \u001b[36mDecodingTask._main_loop\u001b[39m\u001b[34m(self, audio_features, tokens)\u001b[39m\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.sample_len):\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m         logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    689\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    690\u001b[39m             i == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenizer.no_speech \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    691\u001b[39m         ):  \u001b[38;5;66;03m# save no_speech_probs\u001b[39;00m\n\u001b[32m    692\u001b[39m             probs_at_sot = logits[:, \u001b[38;5;28mself\u001b[39m.sot_index].float().softmax(dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/whisper/decoding.py:163\u001b[39m, in \u001b[36mPyTorchInference.logits\u001b[39m\u001b[34m(self, tokens, audio_features)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokens.shape[-\u001b[32m1\u001b[39m] > \u001b[38;5;28mself\u001b[39m.initial_token_length:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# only need to use the last token except in the first forward pass\u001b[39;00m\n\u001b[32m    161\u001b[39m     tokens = tokens[:, -\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/whisper/model.py:242\u001b[39m, in \u001b[36mTextDecoder.forward\u001b[39m\u001b[34m(self, x, xa, kv_cache)\u001b[39m\n\u001b[32m    239\u001b[39m x = x.to(xa.dtype)\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    244\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln(x)\n\u001b[32m    245\u001b[39m logits = (\n\u001b[32m    246\u001b[39m     x @ torch.transpose(\u001b[38;5;28mself\u001b[39m.token_embedding.weight.to(x.dtype), \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    247\u001b[39m ).float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/whisper/model.py:170\u001b[39m, in \u001b[36mResidualAttentionBlock.forward\u001b[39m\u001b[34m(self, x, xa, mask, kv_cache)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cross_attn:\n\u001b[32m    169\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.cross_attn(\u001b[38;5;28mself\u001b[39m.cross_attn_ln(x), xa, kv_cache=kv_cache)[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp_ln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/nn/modules/activation.py:738\u001b[39m, in \u001b[36mGELU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapproximate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapproximate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 3) Transcription du fichier\n",
    "\n",
    "start = time.time()\n",
    "result = model.transcribe(\"final_jo_eurosport.mp3\", language=\"fr\") \n",
    "end = time.time()\n",
    "\n",
    "print(\"transcription terminée\")\n",
    "print(f\"Durée : {end - start:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd8ff29-cf44-4e5d-b63b-31cc4d47f4a8",
   "metadata": {},
   "source": [
    "On sauvegarde en .txt et affiche le résultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df74cc43-1f64-4add-8b66-9d6efeca6238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Sauvegarde\n",
    "\n",
    "start = time.time()\n",
    "with open(\"transcription_whisper.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result[\"text\"])\n",
    "end = time.time()\n",
    "\n",
    "print(\"Transcription terminée !\")\n",
    "print(f\"Durée : {end - start:.2f} secondes\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015703a2-df07-4016-8518-4692a6d7554f",
   "metadata": {},
   "source": [
    "On rajoute la diarization (quel interlocuteur parle) (token : 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) --> on installe le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0174c204-8c80-42d2-8c31-c1d76fe6a079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting inaSpeechSegmenter\n",
      "  Downloading inaspeechsegmenter-0.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: numpy in /opt/python/lib/python3.13/site-packages (from inaSpeechSegmenter) (2.3.5)\n",
      "Requirement already satisfied: pandas in /opt/python/lib/python3.13/site-packages (from inaSpeechSegmenter) (2.3.3)\n",
      "Collecting scikit-image (from inaSpeechSegmenter)\n",
      "  Downloading scikit_image-0.25.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pyannote.core in /opt/python/lib/python3.13/site-packages (from inaSpeechSegmenter) (6.0.1)\n",
      "Requirement already satisfied: matplotlib in /opt/python/lib/python3.13/site-packages (from inaSpeechSegmenter) (3.10.7)\n",
      "Collecting Pyro4 (from inaSpeechSegmenter)\n",
      "  Downloading Pyro4-4.82-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pytextgrid (from inaSpeechSegmenter)\n",
      "  Downloading pytextgrid-0.1.4-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: soundfile in /opt/python/lib/python3.13/site-packages (from inaSpeechSegmenter) (0.13.1)\n",
      "Collecting tensorflow[and-cuda] (from inaSpeechSegmenter)\n",
      "  Downloading tensorflow-2.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting onnxruntime-gpu (from inaSpeechSegmenter)\n",
      "  Downloading onnxruntime_gpu-1.23.2-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/python/lib/python3.13/site-packages (from matplotlib->inaSpeechSegmenter) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/python/lib/python3.13/site-packages (from matplotlib->inaSpeechSegmenter) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/python/lib/python3.13/site-packages (from matplotlib->inaSpeechSegmenter) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/python/lib/python3.13/site-packages (from matplotlib->inaSpeechSegmenter) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/python/lib/python3.13/site-packages (from matplotlib->inaSpeechSegmenter) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/python/lib/python3.13/site-packages (from matplotlib->inaSpeechSegmenter) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /opt/python/lib/python3.13/site-packages (from matplotlib->inaSpeechSegmenter) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/python/lib/python3.13/site-packages (from matplotlib->inaSpeechSegmenter) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/python/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib->inaSpeechSegmenter) (1.17.0)\n",
      "Collecting coloredlogs (from onnxruntime-gpu->inaSpeechSegmenter)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime-gpu->inaSpeechSegmenter)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: protobuf in /opt/python/lib/python3.13/site-packages (from onnxruntime-gpu->inaSpeechSegmenter) (6.33.1)\n",
      "Requirement already satisfied: sympy in /opt/python/lib/python3.13/site-packages (from onnxruntime-gpu->inaSpeechSegmenter) (1.14.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu->inaSpeechSegmenter)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/python/lib/python3.13/site-packages (from pandas->inaSpeechSegmenter) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/python/lib/python3.13/site-packages (from pandas->inaSpeechSegmenter) (2025.2)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/python/lib/python3.13/site-packages (from pyannote.core->inaSpeechSegmenter) (2.4.0)\n",
      "Collecting serpent>=1.27 (from Pyro4->inaSpeechSegmenter)\n",
      "  Downloading serpent-1.42-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: scipy>=1.11.4 in /opt/python/lib/python3.13/site-packages (from scikit-image->inaSpeechSegmenter) (1.16.3)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/python/lib/python3.13/site-packages (from scikit-image->inaSpeechSegmenter) (3.6)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image->inaSpeechSegmenter)\n",
      "  Downloading imageio-2.37.2-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->inaSpeechSegmenter)\n",
      "  Downloading tifffile-2025.10.16-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->inaSpeechSegmenter)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/python/lib/python3.13/site-packages (from soundfile->inaSpeechSegmenter) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /opt/python/lib/python3.13/site-packages (from cffi>=1.0->soundfile->inaSpeechSegmenter) (2.23)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/python/lib/python3.13/site-packages (from sympy->onnxruntime-gpu->inaSpeechSegmenter) (1.3.0)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow[and-cuda]->inaSpeechSegmenter)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow[and-cuda]->inaSpeechSegmenter)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow[and-cuda]->inaSpeechSegmenter)\n",
      "  Downloading gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow[and-cuda]->inaSpeechSegmenter)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow[and-cuda]->inaSpeechSegmenter)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow[and-cuda]->inaSpeechSegmenter)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/python/lib/python3.13/site-packages (from tensorflow[and-cuda]->inaSpeechSegmenter) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /opt/python/lib/python3.13/site-packages (from tensorflow[and-cuda]->inaSpeechSegmenter) (80.9.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow[and-cuda]->inaSpeechSegmenter)\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /opt/python/lib/python3.13/site-packages (from tensorflow[and-cuda]->inaSpeechSegmenter) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/python/lib/python3.13/site-packages (from tensorflow[and-cuda]->inaSpeechSegmenter) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/python/lib/python3.13/site-packages (from tensorflow[and-cuda]->inaSpeechSegmenter) (1.76.0)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow[and-cuda]->inaSpeechSegmenter)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow[and-cuda]->inaSpeechSegmenter)\n",
      "  Downloading keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow[and-cuda]->inaSpeechSegmenter)\n",
      "  Downloading h5py-3.15.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow[and-cuda]->inaSpeechSegmenter)\n",
      "  Downloading ml_dtypes-0.5.4-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: nvidia-cublas-cu12<13.0,>=12.5.3.2 in /opt/python/lib/python3.13/site-packages (from tensorflow[and-cuda]->inaSpeechSegmenter) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12<13.0,>=12.5.82 in /opt/python/lib/python3.13/site-packages (from tensorflow[and-cuda]->inaSpeechSegmenter) (12.8.90)\n",
      "Collecting nvidia-cuda-nvcc-cu12<13.0,>=12.5.82 (from tensorflow[and-cuda]->inaSpeechSegmenter)\n",
      "  Downloading nvidia_cuda_nvcc_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12<13.0,>=12.5.82 in /opt/python/lib/python3.13/site-packages (from tensorflow[and-cuda]->inaSpeechSegmenter) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12<13.0,>=12.5.82 in /opt/python/lib/python3.13/site-packages (from tensorflow[and-cuda]->inaSpeechSegmenter) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12<10.0,>=9.3.0.75 in /opt/python/lib/python3.13/site-packages (from tensorflow[and-cuda]->inaSpeechSegmenter) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cufft-cu12<12.0,>=11.2.3.61 in /opt/python/lib/python3.13/site-packages (from tensorflow[and-cuda]->inaSpeechSegmenter) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12<11.0,>=10.3.6.82 in /opt/python/lib/python3.13/site-packages (from tensorflow[and-cuda]->inaSpeechSegmenter) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12<12.0,>=11.6.3.83 in /opt/python/lib/python3.13/site-packages (from tensorflow[and-cuda]->inaSpeechSegmenter) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12<13.0,>=12.5.1.3 in /opt/python/lib/python3.13/site-packages (from tensorflow[and-cuda]->inaSpeechSegmenter) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-nccl-cu12<3.0,>=2.25.1 in /opt/python/lib/python3.13/site-packages (from tensorflow[and-cuda]->inaSpeechSegmenter) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12<13.0,>=12.5.82 in /opt/python/lib/python3.13/site-packages (from tensorflow[and-cuda]->inaSpeechSegmenter) (12.8.93)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]->inaSpeechSegmenter) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]->inaSpeechSegmenter) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]->inaSpeechSegmenter) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]->inaSpeechSegmenter) (2025.11.12)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow[and-cuda]->inaSpeechSegmenter)\n",
      "  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow[and-cuda]->inaSpeechSegmenter)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/python/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow[and-cuda]->inaSpeechSegmenter) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/python/lib/python3.13/site-packages (from astunparse>=1.6.0->tensorflow[and-cuda]->inaSpeechSegmenter) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/python/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow[and-cuda]->inaSpeechSegmenter) (14.2.0)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow[and-cuda]->inaSpeechSegmenter)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow[and-cuda]->inaSpeechSegmenter)\n",
      "  Downloading optree-0.18.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/python/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow[and-cuda]->inaSpeechSegmenter) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/python/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow[and-cuda]->inaSpeechSegmenter) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/python/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow[and-cuda]->inaSpeechSegmenter) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/python/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow[and-cuda]->inaSpeechSegmenter) (0.1.2)\n",
      "Downloading inaspeechsegmenter-0.8.0-py3-none-any.whl (39 kB)\n",
      "Downloading onnxruntime_gpu-1.23.2-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (300.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.5/300.5 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m  \u001b[33m0:00:12\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n",
      "Downloading serpent-1.42-py3-none-any.whl (9.7 kB)\n",
      "Downloading pytextgrid-0.1.4-py3-none-any.whl (9.1 kB)\n",
      "Downloading scikit_image-0.25.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading imageio-2.37.2-py3-none-any.whl (317 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading tifffile-2025.10.16-py3-none-any.whl (231 kB)\n",
      "Downloading tensorflow-2.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.8/620.8 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m  \u001b[33m0:00:15\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.4-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (40.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.15.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.18.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (414 kB)\n",
      "Installing collected packages: pytextgrid, namex, libclang, flatbuffers, tifffile, termcolor, tensorboard-data-server, serpent, optree, opt_einsum, nvidia-cuda-nvcc-cu12, ml_dtypes, markdown, lazy-loader, imageio, humanfriendly, h5py, google_pasta, gast, astunparse, absl-py, tensorboard, scikit-image, Pyro4, coloredlogs, onnxruntime-gpu, keras, tensorflow, inaSpeechSegmenter\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29/29\u001b[0m [inaSpeechSegmenter]ensorflow]-gpu]-cu12]r]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Pyro4-4.82 absl-py-2.3.1 astunparse-1.6.3 coloredlogs-15.0.1 flatbuffers-25.9.23 gast-0.7.0 google_pasta-0.2.0 h5py-3.15.1 humanfriendly-10.0 imageio-2.37.2 inaSpeechSegmenter-0.8.0 keras-3.12.0 lazy-loader-0.4 libclang-18.1.1 markdown-3.10 ml_dtypes-0.5.4 namex-0.1.0 nvidia-cuda-nvcc-cu12-12.9.86 onnxruntime-gpu-1.23.2 opt_einsum-3.4.0 optree-0.18.0 pytextgrid-0.1.4 scikit-image-0.25.2 serpent-1.42 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.2.0 tifffile-2025.10.16\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "!pip install inaSpeechSegmenter\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c6289-4b13-4ec8-86f9-73749c398ba8",
   "metadata": {},
   "source": [
    "Diarisation du dialogue :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ba43dd5-bb41-4d96-aaa7-17cd0b0eb59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 15:57:53.733662: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-01 15:57:53.814730: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-01 15:57:56.075342: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/ina-foss/inaSpeechSegmenter/releases/download/models/keras_speech_music_noise_cnn.hdf5\n",
      "\u001b[1m3244808/3244808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/lib/python3.13/site-packages/keras/src/layers/reshaping/reshape.py:38: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "2025-12-01 15:57:57.487391: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/ina-foss/inaSpeechSegmenter/releases/download/models/keras_male_female_cnn.hdf5\n",
      "\u001b[1m6040200/6040200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "46/46 - 1s - 27ms/step\n",
      "46/46 - 2s - 43ms/step\n",
      "female 0.0 12.88\n",
      "male 12.88 21.84\n",
      "noEnergy 21.84 22.36\n",
      "male 22.36 26.580000000000002\n",
      "noEnergy 26.580000000000002 27.7\n",
      "male 27.7 29.46\n",
      "noEnergy 29.46 30.42\n",
      "male 30.42 31.46\n",
      "noEnergy 31.46 31.94\n"
     ]
    }
   ],
   "source": [
    "from inaSpeechSegmenter import Segmenter\n",
    "\n",
    "segmenter = Segmenter(vad_engine=\"smn\", detect_gender=True)\n",
    "segments = segmenter(\"finale_100m_H.wav\")\n",
    "\n",
    "for label, start, end in segments:\n",
    "    print(label, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c3c225-e758-4e25-85e1-1598d5a228f8",
   "metadata": {},
   "source": [
    "Sauvergarde de la diarization dans un csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a262065-68fb-4f91-9827-f11f5559191f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV créé : segments_final_jo.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>12.88</td>\n",
       "      <td>21.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>noEnergy</td>\n",
       "      <td>21.84</td>\n",
       "      <td>22.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>22.36</td>\n",
       "      <td>26.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>noEnergy</td>\n",
       "      <td>26.58</td>\n",
       "      <td>27.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  start    end\n",
       "0    female   0.00  12.88\n",
       "1      male  12.88  21.84\n",
       "2  noEnergy  21.84  22.36\n",
       "3      male  22.36  26.58\n",
       "4  noEnergy  26.58  27.70"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 'segments' est la liste des timecodes\n",
    "df_dia = pd.DataFrame(segments, columns=[\"label\", \"start\", \"end\"])\n",
    "\n",
    "df_dia.to_csv(\"segments_final_jo.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"CSV créé : segments_final_jo.csv\")\n",
    "df_dia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead40b1-6614-4538-a962-a3133ed07ec1",
   "metadata": {},
   "source": [
    "On refait la transcription de l'audio en texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3c05fcd-f1f6-43c9-88b9-fee00bce0042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transcription terminée\n",
      "Durée : 87.51 secondes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "result = model.transcribe(\"finale_100m_H.wav\", language=\"fr\", fp16=False)\n",
    "end = time.time()\n",
    "\n",
    "print(\"transcription terminée\")\n",
    "print(f\"Durée : {end - start:.2f} secondes\")\n",
    "\n",
    "texte = result[\"text\"]\n",
    "with open(\"transcription_whisper_final_jo.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "112867e3-4dd2-4d0c-b624-edb2d1cdc23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le président de la République qui a un téléphone caché avec un faux nom, alors les plus gentils vont dire c'est une affaire de cours de récréation, puis les plus méchants vont dire c'est une affaire de voyous, c'est un des méthodes de voyous, c'est très étrange quand même. Madame, faites très attention au vocabulaire que vous employez, parce que je ne suis pas quelqu'un qui plaisante, donc vous ne parlez pas de moi comme un voyou. Pardon de le dire, parce que moi je pourrais dire aussi qu'inviter M. Takedin chez BFM comme vous l'avez fait, c'est une affaire de voyous. Donc mesurez votre vocabulaire. Et les choses iront beaucoup mieux.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63799a2-e02f-4d42-a565-67919f01364b",
   "metadata": {},
   "source": [
    "À ce stade, on a d'un côté le texte, de l'autre le time code des interlocuteurs. Objectif, les fusioner pour obtenir quelque chose du type : \n",
    "\n",
    "Interlocuteur_1 : \"Bla bla bla\"\n",
    "'Silence'\n",
    "Interlocuteur_2 : \"Blo blo blo\"\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d3c3b-c077-441a-80c8-b54122641f16",
   "metadata": {},
   "source": [
    "Pour ce faire, on met les segments détectés par Whisper dans un df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f04bace-5449-49d3-8c46-0085696260b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>4.70</td>\n",
       "      <td>Le président de la République qui a un téléph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.04</td>\n",
       "      <td>8.10</td>\n",
       "      <td>alors les plus gentils vont dire c'est une af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.20</td>\n",
       "      <td>10.40</td>\n",
       "      <td>puis les plus méchants vont dire c'est une af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.54</td>\n",
       "      <td>12.70</td>\n",
       "      <td>c'est un des méthodes de voyous, c'est très é...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.72</td>\n",
       "      <td>15.92</td>\n",
       "      <td>Madame, faites très attention au vocabulaire ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start    end                                               text\n",
       "0   0.00   4.70   Le président de la République qui a un téléph...\n",
       "1   5.04   8.10   alors les plus gentils vont dire c'est une af...\n",
       "2   8.20  10.40   puis les plus méchants vont dire c'est une af...\n",
       "3  10.54  12.70   c'est un des méthodes de voyous, c'est très é...\n",
       "4  12.72  15.92   Madame, faites très attention au vocabulaire ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_segments = result[\"segments\"]\n",
    "\n",
    "rows = []\n",
    "for seg in whisper_segments:\n",
    "    rows.append({\n",
    "        \"start\": seg[\"start\"],\n",
    "        \"end\": seg[\"end\"],\n",
    "        \"text\": seg[\"text\"]\n",
    "    })\n",
    "\n",
    "df_txt = pd.DataFrame(rows)\n",
    "df_txt = df_txt.sort_values(\"start\").reset_index(drop=True)\n",
    "\n",
    "df_txt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3471eea5-c5d9-4419-94e0-8770725e817b",
   "metadata": {},
   "source": [
    "On associe les segments détectés par Whisper à ceux détectés par Segmenteur (la diarisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9bbd1ee-535e-4add-9b3c-a8605a833793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "      <th>mid</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>4.70</td>\n",
       "      <td>Le président de la République qui a un téléph...</td>\n",
       "      <td>2.35</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.04</td>\n",
       "      <td>8.10</td>\n",
       "      <td>alors les plus gentils vont dire c'est une af...</td>\n",
       "      <td>6.57</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.20</td>\n",
       "      <td>10.40</td>\n",
       "      <td>puis les plus méchants vont dire c'est une af...</td>\n",
       "      <td>9.30</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.54</td>\n",
       "      <td>12.70</td>\n",
       "      <td>c'est un des méthodes de voyous, c'est très é...</td>\n",
       "      <td>11.62</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.72</td>\n",
       "      <td>15.92</td>\n",
       "      <td>Madame, faites très attention au vocabulaire ...</td>\n",
       "      <td>14.32</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start    end                                               text    mid  \\\n",
       "0   0.00   4.70   Le président de la République qui a un téléph...   2.35   \n",
       "1   5.04   8.10   alors les plus gentils vont dire c'est une af...   6.57   \n",
       "2   8.20  10.40   puis les plus méchants vont dire c'est une af...   9.30   \n",
       "3  10.54  12.70   c'est un des méthodes de voyous, c'est très é...  11.62   \n",
       "4  12.72  15.92   Madame, faites très attention au vocabulaire ...  14.32   \n",
       "\n",
       "  speaker  \n",
       "0  female  \n",
       "1  female  \n",
       "2  female  \n",
       "3  female  \n",
       "4    male  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_speaker(t, df_dia):\n",
    "    match = df_dia[(df_dia[\"start\"] <= t) & (df_dia[\"end\"] >= t)]\n",
    "    if len(match) == 0:\n",
    "        return None\n",
    "    return match.iloc[0][\"label\"]\n",
    "\n",
    "df_txt[\"mid\"] = (df_txt[\"start\"] + df_txt[\"end\"]) / 2\n",
    "df_txt[\"speaker\"] = df_txt[\"mid\"].apply(lambda t: get_speaker(t, df_dia))\n",
    "\n",
    "df_txt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb6144-726f-405a-973d-c5bdf9a1e032",
   "metadata": {},
   "source": [
    "On sauvegarde ce résultat dans un csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c646b6a-3886-481e-b20c-786b85b441a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txt.to_csv(\"whisper_avec_speaker.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d34b1c-6f92-42d7-a1f9-947e67f6d7d0",
   "metadata": {},
   "source": [
    "On affiche le résultat : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44c40721-c076-46c9-ae50-838268d7fbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.0s] female : Le président de la République qui a un téléphone caché avec un faux nom,\n",
      "[   5.0s] female : alors les plus gentils vont dire c'est une affaire de cours de récréation,\n",
      "[   8.2s] female : puis les plus méchants vont dire c'est une affaire de voyous,\n",
      "[  10.5s] female : c'est un des méthodes de voyous, c'est très étrange quand même.\n",
      "[  12.7s] male : Madame, faites très attention au vocabulaire que vous employez,\n",
      "[  16.1s] male : parce que je ne suis pas quelqu'un qui plaisante,\n",
      "[  18.2s] male : donc vous ne parlez pas de moi comme un voyou.\n",
      "[  20.0s] male : Pardon de le dire, parce que moi je pourrais dire aussi\n",
      "[  21.8s] male : qu'inviter M. Takedin chez BFM comme vous l'avez fait,\n",
      "[  25.6s] male : c'est une affaire de voyous.\n"
     ]
    }
   ],
   "source": [
    "with open(\"dialogue_avec_speaker.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(10):  # affiche les 10 premières lignes\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae832d73-fa15-4ea7-822e-fe8524f4e143",
   "metadata": {},
   "source": [
    "Maintenant que notre méthode est au point, on télécharge toutes les vidéos des JOs (FranceTV et Eurosport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bb7edec-5789-4273-b429-453b0200d746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yt_dlp\n",
      "  Downloading yt_dlp-2025.11.12-py3-none-any.whl.metadata (180 kB)\n",
      "Downloading yt_dlp-2025.11.12-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: yt_dlp\n",
      "Successfully installed yt_dlp-2025.11.12\n"
     ]
    }
   ],
   "source": [
    "!pip install yt_dlp\n",
    "import yt_dlp\n",
    "import s3fs\n",
    "import os\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = 'TFM0GSFSIN0GLN2OZMIK'\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = 'qkhO8rOem3zuKQf21YuXMh14K5kJ7RB+RKf0DLrN'\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = 'eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NLZXkiOiJURk0wR1NGU0lOMEdMTjJPWk1JSyIsImFsbG93ZWQtb3JpZ2lucyI6WyIqIl0sImF1ZCI6WyJtaW5pby1kYXRhbm9kZSIsIm9ueXhpYSIsImFjY291bnQiXSwiYXV0aF90aW1lIjoxNzYyMzMxMjgxLCJhenAiOiJvbnl4aWEiLCJlbWFpbCI6Imx1Y2FzLmN1bXVuZWxAZW5zYWUuZnIiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiZXhwIjoxNzYzODI1NjU5LCJmYW1pbHlfbmFtZSI6IkN1bXVuZWwiLCJnaXZlbl9uYW1lIjoiTHVjYXMiLCJncm91cHMiOlsiVVNFUl9PTllYSUEiLCJzdGF0YXBwLXNlZ21lZGljIl0sImlhdCI6MTc2MzIyMDg1OSwiaXNzIjoiaHR0cHM6Ly9hdXRoLmxhYi5zc3BjbG91ZC5mci9hdXRoL3JlYWxtcy9zc3BjbG91ZCIsImp0aSI6Im9ucnRydDpmYzZkMTEzZC03MGE5LTBlNzItNzViNC03ZGRiZDRkYWU1MWQiLCJuYW1lIjoiTHVjYXMgQ3VtdW5lbCIsInBvbGljeSI6InN0c29ubHkiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJsYWIiLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiIsImRlZmF1bHQtcm9sZXMtc3NwY2xvdWQiXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJyb2xlcyI6WyJvZmZsaW5lX2FjY2VzcyIsInVtYV9hdXRob3JpemF0aW9uIiwiZGVmYXVsdC1yb2xlcy1zc3BjbG91ZCJdLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGdyb3VwcyBlbWFpbCIsInNpZCI6ImNkZjM5NGE5LWQzZTMtNGZjZC05Y2E0LWUyNTU0ZTk5ODIwMyIsInN1YiI6ImUyZDc4NjRjLTcwMzItNDI0ZC04OTA2LWU0ZjhiNDFjYzAwMyIsInR5cCI6IkJlYXJlciJ9.cZHqmlEXSfirQKfAIlWGZw1kDYG5oV2uVwkZxTpjXgU9OR3L2g_iiNLvcFR4xPud14pSTJqchmEMRyZW5CyBKw'\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = 'us-east-1'\n",
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},\n",
    "    key = os.environ[\"AWS_ACCESS_KEY_ID\"], \n",
    "    secret = os.environ[\"AWS_SECRET_ACCESS_KEY\"], \n",
    "    token = os.environ[\"AWS_SESSION_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d712193e-5869-4352-8b9e-11ef1a4b4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "import re\n",
    "import os\n",
    "\n",
    "fs = s3fs.S3FileSystem()  # your credentials are already filled\n",
    "#playlist_url = \"https://www.youtube.com/playlist?list=PLYXOi0ZKJEKKEWl_jgqTTXN9mzvcj4oP0\" #FTV\n",
    "playlist_url = \"https://www.youtube.com/playlist?list=PLMvsN4LH5cSFtlag_bbqDc76pyetG_4Ol\" #ESP\n",
    "ydl_opts = {'quiet': True, 'extract_flat': True}\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    info = ydl.extract_info(playlist_url, download=False)\n",
    "\n",
    "playlist_dict = {entry['title']: entry for entry in info['entries']}\n",
    "\n",
    "# Start from index 340\n",
    "entries_to_download = list(playlist_dict.items())[886:]\n",
    "\n",
    "for title, entry in entries_to_download:\n",
    "    # Skip long videos (>25 min)\n",
    "    if entry.get('duration') and entry['duration'] > 1500:\n",
    "        continue\n",
    "\n",
    "    # Safe title for filenames\n",
    "    safe_title = re.sub(r'[^a-zA-Z0-9_\\- ]', '', title)[:100].strip()\n",
    "    \n",
    "    local_path = f\"/tmp/{safe_title}.webm\"\n",
    "    s3_path = f\"s3://lab/PESSD/ESP/{safe_title}.webm\"\n",
    "\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio[ext=webm]',\n",
    "        'outtmpl': local_path,\n",
    "        'postprocessors': [],\n",
    "        'quiet': True,\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([entry['url']])\n",
    "\n",
    "    # Upload to S3\n",
    "    fs.put(local_path, s3_path)\n",
    "\n",
    "    # Delete local temp file\n",
    "    os.remove(local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500273ac-b61a-4ac1-819d-906f1d97ac25",
   "metadata": {},
   "source": [
    "Le but est désormais de généraliser la méthode créée ci-dessus à l'ensemble des audios obtenus ici.\n",
    "\n",
    "On commence par charger les modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e338a2ed-bfa6-46e6-ac41-9956efb3fc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 17:07:12.330952: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-01 17:07:12.411958: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-01 17:07:14.161049: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/opt/python/lib/python3.13/site-packages/keras/src/layers/reshaping/reshape.py:38: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "2025-12-01 17:07:21.813871: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "étape terminée\n"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "import re\n",
    "import os\n",
    "#import s3fs\n",
    "import pandas as pd\n",
    "from inaSpeechSegmenter import Segmenter\n",
    "import whisper\n",
    "\n",
    "#fs = s3fs.S3FileSystem()  # déjà configuré chez toi\n",
    "\n",
    "# 1) Charger Whisper (choisis le modèle que tu utilises déjà)\n",
    "whisper_model = whisper.load_model(\"small\")\n",
    "\n",
    "# 2) Charger le segmenter inaSpeech (avec genre)\n",
    "segmenter = Segmenter(vad_engine=\"smn\", detect_gender=True)\n",
    "\n",
    "print(\"étape terminée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21008f3a-cee7-4e37-85f9-c6df56ecb609",
   "metadata": {},
   "source": [
    "On crée une fonction qui fait transcription (segmentée) + diarisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b50ddb8-aba8-4f49-9127-a8f8176b67c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_and_diarize(local_audio_path):\n",
    "    # --- Whisper ---\n",
    "    result = whisper_model.transcribe(local_audio_path, language=\"fr\", fp16=False)\n",
    "    text = result[\"text\"]\n",
    "    \n",
    "    # DataFrame des segments Whisper\n",
    "    rows = []\n",
    "    for seg in result[\"segments\"]:\n",
    "        rows.append({\n",
    "            \"start\": seg[\"start\"],\n",
    "            \"end\": seg[\"end\"],\n",
    "            \"text\": seg[\"text\"]\n",
    "        })\n",
    "    df_txt = pd.DataFrame(rows).sort_values(\"start\").reset_index(drop=True)\n",
    "\n",
    "    # --- inaSpeechSegmenter ---\n",
    "    segments = segmenter(local_audio_path)  # liste (label, start, end)\n",
    "    df_dia = pd.DataFrame(segments, columns=[\"label\", \"start\", \"end\"])\\\n",
    "              .sort_values(\"start\").reset_index(drop=True)\n",
    "\n",
    "    # --- associer chaque segment de texte à un \"speaker\" ---\n",
    "    def get_speaker(t):\n",
    "        match = df_dia[(df_dia[\"start\"] <= t) & (df_dia[\"end\"] >= t)]\n",
    "        if len(match) == 0:\n",
    "            return None\n",
    "        return match.iloc[0][\"label\"]\n",
    "\n",
    "    df_txt[\"mid\"] = (df_txt[\"start\"] + df_txt[\"end\"]) / 2\n",
    "    df_txt[\"speaker\"] = df_txt[\"mid\"].apply(get_speaker)\n",
    "\n",
    "    return text, df_txt, df_dia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2620c0-d2d9-4997-a1a7-77ccb7f90104",
   "metadata": {},
   "source": [
    "On applique notre fonction aux vidéos de la playlist youtube en l'appliquant à la boucle de Lucas.\n",
    "\n",
    "\n",
    "On commence par un test pour voire combien de temps ça va prendre : on applique notre boucle uniquement à 4 vidéos pour commencer. Par ailleurs pour ce test, on n'utilise pas S3 qui plante ici, on fait tout en local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b69ccc51-5319-4bdd-8aed-713a66530d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de vidéos dans la playlist : 886\n"
     ]
    }
   ],
   "source": [
    "# Je regarde juste quelle taille fait la playlist histoire de vérifier que c'est bien la bonne/qu'elle n'est pas vide.\n",
    "# ---- Playlist ----\n",
    "playlist_url = \"https://www.youtube.com/playlist?list=PLMvsN4LH5cSFtlag_bbqDc76pyetG_4Ol\"\n",
    "\n",
    "ydl_opts = {'quiet': True, 'extract_flat': True}\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    info = ydl.extract_info(playlist_url, download=False)\n",
    "\n",
    "playlist_dict = {entry['title']: entry for entry in info['entries']}\n",
    "\n",
    "entries = list(playlist_dict.items())\n",
    "print(\"Nombre total de vidéos dans la playlist :\", len(entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56e1673c-ba28-4179-ad10-c395660bbb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de vidéos pour le test : 5\n",
      "\n",
      "[1/5] Vidéo : JO PARIS 2024 - Irrespirable : Médaille d'argent pour les Bleues, perdantes magnifiques contre USA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] No supported JavaScript runtime could be found. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one. To silence this warning, you can use  --extractor-args \"youtube:player_client=default\"\n",
      "WARNING: [youtube] R8U0u-gIKe0: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
      "WARNING: [youtube] R8U0u-gIKe0: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Audio téléchargé : ./out_esp/JO PARIS 2024 - Irrespirable  Mdaille dargent pour les Bleues perdantes magnifiques contre USA.webm\n",
      "220/220 - 3s - 14ms/step\n",
      "188/188 - 6s - 30ms/step\n",
      "  ✓ Vidéo traitée en 84.4 secondes\n",
      "    → Transcription : ./out_esp/JO PARIS 2024 - Irrespirable  Mdaille dargent pour les Bleues perdantes magnifiques contre USA.txt\n",
      "    → Segments dia  : ./out_esp/JO PARIS 2024 - Irrespirable  Mdaille dargent pour les Bleues perdantes magnifiques contre USA_segments.csv\n",
      "    → Segments+spk  : ./out_esp/JO PARIS 2024 - Irrespirable  Mdaille dargent pour les Bleues perdantes magnifiques contre USA_with_speaker.csv\n",
      "\n",
      "[2/5] Vidéo : JO PARIS 2024 - Élodie Clouvel décroche la médaille d'argent au pentathlon moderne\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] No supported JavaScript runtime could be found. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one. To silence this warning, you can use  --extractor-args \"youtube:player_client=default\"\n",
      "WARNING: [youtube] 7k23ukTeh_U: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
      "WARNING: [youtube] 7k23ukTeh_U: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Audio téléchargé : ./out_esp/JO PARIS 2024 - lodie Clouvel dcroche la mdaille dargent au pentathlon moderne.webm\n",
      "205/205 - 2s - 12ms/step\n",
      "165/165 - 5s - 32ms/step\n",
      "  ✓ Vidéo traitée en 107.4 secondes\n",
      "    → Transcription : ./out_esp/JO PARIS 2024 - lodie Clouvel dcroche la mdaille dargent au pentathlon moderne.txt\n",
      "    → Segments dia  : ./out_esp/JO PARIS 2024 - lodie Clouvel dcroche la mdaille dargent au pentathlon moderne_segments.csv\n",
      "    → Segments+spk  : ./out_esp/JO PARIS 2024 - lodie Clouvel dcroche la mdaille dargent au pentathlon moderne_with_speaker.csv\n",
      "\n",
      "[3/5] Vidéo : JO PARIS 2024 - La Néerlandaise Sifan Hassan remporte le marathon avec un record olympique à la clé\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] No supported JavaScript runtime could be found. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one. To silence this warning, you can use  --extractor-args \"youtube:player_client=default\"\n",
      "WARNING: [youtube] ejs4c9-6eDA: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
      "WARNING: [youtube] ejs4c9-6eDA: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Audio téléchargé : ./out_esp/JO PARIS 2024 - La Nerlandaise Sifan Hassan remporte le marathon avec un record olympique  la cl.webm\n",
      "211/211 - 3s - 15ms/step\n",
      "114/114 - 3s - 27ms/step\n",
      "  ✓ Vidéo traitée en 143.2 secondes\n",
      "    → Transcription : ./out_esp/JO PARIS 2024 - La Nerlandaise Sifan Hassan remporte le marathon avec un record olympique  la cl.txt\n",
      "    → Segments dia  : ./out_esp/JO PARIS 2024 - La Nerlandaise Sifan Hassan remporte le marathon avec un record olympique  la cl_segments.csv\n",
      "    → Segments+spk  : ./out_esp/JO PARIS 2024 - La Nerlandaise Sifan Hassan remporte le marathon avec un record olympique  la cl_with_speaker.csv\n",
      "\n",
      "[4/5] Vidéo : JO PARIS 2024 - Émotions fortes, chutes lourdes, images marquantes : Le best of du 15e jour\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] No supported JavaScript runtime could be found. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one. To silence this warning, you can use  --extractor-args \"youtube:player_client=default\"\n",
      "WARNING: [youtube] 5iIyJY7qgik: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
      "WARNING: [youtube] 5iIyJY7qgik: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Audio téléchargé : ./out_esp/JO PARIS 2024 - motions fortes chutes lourdes images marquantes  Le best of du 15e jour.webm\n",
      "156/156 - 2s - 12ms/step\n",
      "36/36 - 1s - 30ms/step\n",
      "  ✓ Vidéo traitée en 60.3 secondes\n",
      "    → Transcription : ./out_esp/JO PARIS 2024 - motions fortes chutes lourdes images marquantes  Le best of du 15e jour.txt\n",
      "    → Segments dia  : ./out_esp/JO PARIS 2024 - motions fortes chutes lourdes images marquantes  Le best of du 15e jour_segments.csv\n",
      "    → Segments+spk  : ./out_esp/JO PARIS 2024 - motions fortes chutes lourdes images marquantes  Le best of du 15e jour_with_speaker.csv\n",
      "\n",
      "[5/5] Vidéo : JO PARIS 2024 - Tout juste vaincu contre Team USA, Wembanyama donne déjà rendez-vous pour l'avenir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] No supported JavaScript runtime could be found. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one. To silence this warning, you can use  --extractor-args \"youtube:player_client=default\"\n",
      "WARNING: [youtube] H3xB_ne10vM: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
      "WARNING: [youtube] H3xB_ne10vM: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Audio téléchargé : ./out_esp/JO PARIS 2024 - Tout juste vaincu contre Team USA Wembanyama donne dj rendez-vous pour lavenir.webm\n",
      "197/197 - 2s - 12ms/step\n",
      "191/191 - 5s - 28ms/step\n",
      "  ✓ Vidéo traitée en 87.6 secondes\n",
      "    → Transcription : ./out_esp/JO PARIS 2024 - Tout juste vaincu contre Team USA Wembanyama donne dj rendez-vous pour lavenir.txt\n",
      "    → Segments dia  : ./out_esp/JO PARIS 2024 - Tout juste vaincu contre Team USA Wembanyama donne dj rendez-vous pour lavenir_segments.csv\n",
      "    → Segments+spk  : ./out_esp/JO PARIS 2024 - Tout juste vaincu contre Team USA Wembanyama donne dj rendez-vous pour lavenir_with_speaker.csv\n",
      "\n",
      "=== Test terminé ===\n",
      "Durée totale (4 vidéos) : 483.0 secondes\n",
      "Fichiers générés dans : ./out_esp\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "import yt_dlp\n",
    "import pandas as pd\n",
    "# plus besoin de s3fs ici\n",
    "\n",
    "# ---- Dossier de sortie local ----\n",
    "OUTPUT_DIR = \"./out_esp\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---- Playlist ----\n",
    "playlist_url = \"https://www.youtube.com/playlist?list=PLMvsN4LH5cSFtlag_bbqDc76pyetG_4Ol\"\n",
    "\n",
    "ydl_opts = {'quiet': True, 'extract_flat': True}\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    info = ydl.extract_info(playlist_url, download=False)\n",
    "\n",
    "playlist_dict = {entry['title']: entry for entry in info['entries']}\n",
    "\n",
    "# ---- TEST : ne traiter que 4 vidéos ----\n",
    "entries_to_download = list(playlist_dict.items())[100:105]   # >>> EXACTEMENT 5 vidéos <<<\n",
    "\n",
    "print(f\"Nombre de vidéos pour le test : {len(entries_to_download)}\")\n",
    "\n",
    "t_global_start = time.time()\n",
    "\n",
    "for idx, (title, entry) in enumerate(entries_to_download, start=1):\n",
    "\n",
    "    print(f\"\\n[{idx}/{len(entries_to_download)}] Vidéo : {title}\")\n",
    "\n",
    "    duration = entry.get('duration')\n",
    "    if duration and duration > 1500:   # skip >25 min\n",
    "        print(\"  → SKIP (trop long)\")\n",
    "        continue\n",
    "\n",
    "    # Nom de fichier safe\n",
    "    safe_title = re.sub(r'[^a-zA-Z0-9_\\- ]', '', title)[:100].strip()\n",
    "    if not safe_title:\n",
    "        safe_title = f\"video_{idx}\"\n",
    "\n",
    "    # Chemins locaux dans OUTPUT_DIR\n",
    "    local_audio = os.path.join(OUTPUT_DIR, f\"{safe_title}.webm\")\n",
    "    local_txt   = os.path.join(OUTPUT_DIR, f\"{safe_title}.txt\")\n",
    "    local_seg   = os.path.join(OUTPUT_DIR, f\"{safe_title}_segments.csv\")\n",
    "    local_spk   = os.path.join(OUTPUT_DIR, f\"{safe_title}_with_speaker.csv\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # ---- 1) Télécharger audio ----\n",
    "    try:\n",
    "        ydl_opts = {\n",
    "            'format': 'bestaudio[ext=webm]',\n",
    "            'outtmpl': local_audio,\n",
    "            'postprocessors': [],\n",
    "            'quiet': True,\n",
    "        }\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([entry['url']])\n",
    "        print(\"  → Audio téléchargé :\", local_audio)\n",
    "    except Exception as e:\n",
    "        print(f\"  ERREUR DOWNLOAD : {e}\")\n",
    "        continue\n",
    "\n",
    "    # ---- 2) Transcription + diarisation ----\n",
    "    try:\n",
    "        text, df_txt, df_dia = transcribe_and_diarize(local_audio)\n",
    "    except Exception as e:\n",
    "        print(f\"  ERREUR Whisper/inaSpeech : {e}\")\n",
    "        continue\n",
    "\n",
    "    # ---- 3) Sauvegarde locale ----\n",
    "    with open(local_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    df_dia.to_csv(local_seg, index=False, encoding=\"utf-8\")\n",
    "    df_txt.to_csv(local_spk, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f\"  ✓ Vidéo traitée en {t1 - t0:.1f} secondes\")\n",
    "    print(f\"    → Transcription : {local_txt}\")\n",
    "    print(f\"    → Segments dia  : {local_seg}\")\n",
    "    print(f\"    → Segments+spk  : {local_spk}\")\n",
    "\n",
    "t_global_end = time.time()\n",
    "print(\"\\n=== Test terminé ===\")\n",
    "print(f\"Durée totale (5 vidéos) : {t_global_end - t_global_start:.1f} secondes\")\n",
    "print(\"Fichiers générés dans :\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b59d39-b495-441d-8a2c-3aa9aead122e",
   "metadata": {},
   "source": [
    "On lit un des fichiers créés pour voir si ça a bien transcrit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c956da60-9628-463b-b9e2-2e389c295b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "      <th>mid</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.64</td>\n",
       "      <td>Attention, l'angoisse est arrivée.</td>\n",
       "      <td>1.32</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.64</td>\n",
       "      <td>4.64</td>\n",
       "      <td>Il est resté dans son tir.</td>\n",
       "      <td>3.64</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.64</td>\n",
       "      <td>6.14</td>\n",
       "      <td>Allez, Lodi.</td>\n",
       "      <td>5.39</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.14</td>\n",
       "      <td>9.14</td>\n",
       "      <td>On retouche la table à chaque fois pour avant...</td>\n",
       "      <td>7.64</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.14</td>\n",
       "      <td>12.14</td>\n",
       "      <td>Et voilà, l'angoisse a compris, elle va être ...</td>\n",
       "      <td>10.64</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start    end                                               text    mid  \\\n",
       "0   0.00   2.64                 Attention, l'angoisse est arrivée.   1.32   \n",
       "1   2.64   4.64                         Il est resté dans son tir.   3.64   \n",
       "2   4.64   6.14                                       Allez, Lodi.   5.39   \n",
       "3   6.14   9.14   On retouche la table à chaque fois pour avant...   7.64   \n",
       "4   9.14  12.14   Et voilà, l'angoisse a compris, elle va être ...  10.64   \n",
       "\n",
       "  speaker  \n",
       "0    male  \n",
       "1  female  \n",
       "2  female  \n",
       "3    male  \n",
       "4    male  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, pandas as pd\n",
    "\n",
    "os.listdir(\"./out_esp\")\n",
    "df = pd.read_csv(\"./out_esp/JO PARIS 2024 - lodie Clouvel dcroche la mdaille dargent au pentathlon moderne_with_speaker.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebb3f384-3777-42b8-b3db-aa8fff67dcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier dialogue créé : ./out_esp/JO PARIS 2024 - lodie Clouvel dcroche la mdaille dargent au pentathlon moderne_dialogue.txt\n",
      "[   0.0s] male     : Attention, l'angoisse est arrivée.\n",
      "[   2.6s] female   : Il est resté dans son tir. Allez, Lodi.\n",
      "[   6.1s] male     : On retouche la table à chaque fois pour avant de retirer. Et voilà, l'angoisse a compris, elle va être championne olympique. Elle va être championne olympique. Non, Lodi, ça fait deux, trois. Allez, allez chercher l'argent. Le dernier.\n",
      "[  18.6s] music    : Allez, allez, allez, bravo Lodi, allez. Tu l'as bien mérité.\n",
      "[  24.6s] female   : Une deuxième médaille d'argent, huit ans après Rio. C'est magnifique.\n",
      "[  29.1s] male     : Allez, la sud-coréenne qui va repartir sans doute trop loin pour aller chercher l'Odic Louvelle pour l'argent.\n",
      "[  35.1s] music    : Mais la médaille, allez, il se poquette à Delle. Allez, allez, allez.\n",
      "[  43.1s] female   : Je suis dépassée par l'émotion, mais c'est magnifique. Vous n'imaginez pas comme... C'est magnifique de l'avoir par ça, de l'avoir réussir à domicile. Elle s'est vraiment accrochée. Valentin Bolo, son conjoint, qui ne s'est pas qualifié. Et qui est là, qui est là en train de la regarder, réaliser cet exploit, aller chercher cette médaille d'argent. 300 mètres.\n",
      "[  65.5s] male     : C'est magnifique. Allez, 300 mètres maintenant pour la hongroise, qui elle va aller chercher la médaille d'or, le titre olympique et l'Odic Louvelle a passé. Les 300 mètres aussi lui restent 300 mètres pour aller chercher la médaille d'argent.\n",
      "[  77.5s] female   : Oui. Cette médaille d'argent, bravo Lodi. Là, elle peut y aller, elle peut profiter.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def make_dialogue_file(df, output_path, merge_same_speaker=True, max_gap=1.0):\n",
    "    \"\"\"\n",
    "    df : DataFrame avec colonnes ['start', 'end', 'text', 'speaker']\n",
    "    output_path : chemin du fichier .txt à créer\n",
    "    merge_same_speaker : si True, fusionne les segments consécutifs du même speaker\n",
    "    max_gap : durée max (en secondes) entre deux segments pour les fusionner\n",
    "    \"\"\"\n",
    "\n",
    "    # tri par temps de début, au cas où\n",
    "    df = df.sort_values(\"start\").reset_index(drop=True)\n",
    "\n",
    "    # remplace NaN speaker par UNK\n",
    "    df[\"speaker\"] = df[\"speaker\"].fillna(\"UNK\")\n",
    "\n",
    "    # éventuellement fusionner les segments consécutifs du même speaker\n",
    "    merged_rows = []\n",
    "    if merge_same_speaker and len(df) > 0:\n",
    "        current = {\n",
    "            \"start\": df.loc[0, \"start\"],\n",
    "            \"end\": df.loc[0, \"end\"],\n",
    "            \"speaker\": df.loc[0, \"speaker\"],\n",
    "            \"text\": str(df.loc[0, \"text\"]).strip()\n",
    "        }\n",
    "\n",
    "        for i in range(1, len(df)):\n",
    "            row = df.loc[i]\n",
    "            gap = row[\"start\"] - current[\"end\"]\n",
    "\n",
    "            same_speaker = (row[\"speaker\"] == current[\"speaker\"])\n",
    "            close_in_time = (gap >= 0) and (gap <= max_gap)\n",
    "\n",
    "            if same_speaker and close_in_time:\n",
    "                # on fusionne : on étend la fin et on concatène le texte\n",
    "                current[\"end\"] = row[\"end\"]\n",
    "                current[\"text\"] += \" \" + str(row[\"text\"]).strip()\n",
    "            else:\n",
    "                merged_rows.append(current)\n",
    "                current = {\n",
    "                    \"start\": row[\"start\"],\n",
    "                    \"end\": row[\"end\"],\n",
    "                    \"speaker\": row[\"speaker\"],\n",
    "                    \"text\": str(row[\"text\"]).strip()\n",
    "                }\n",
    "\n",
    "        merged_rows.append(current)\n",
    "    else:\n",
    "        # pas de fusion : on prend tel quel\n",
    "        for _, row in df.iterrows():\n",
    "            merged_rows.append({\n",
    "                \"start\": row[\"start\"],\n",
    "                \"end\": row[\"end\"],\n",
    "                \"speaker\": row[\"speaker\"],\n",
    "                \"text\": str(row[\"text\"]).strip()\n",
    "            })\n",
    "\n",
    "    # écriture dans le fichier texte\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for row in merged_rows:\n",
    "            start = row[\"start\"]\n",
    "            speaker = row[\"speaker\"]\n",
    "            text = row[\"text\"].strip()\n",
    "            # format [ 12.3s] SPEAKER :\n",
    "            f.write(f\"[{start:6.1f}s] {speaker:<8} : {text}\\n\")\n",
    "\n",
    "    print(f\"Fichier dialogue créé : {output_path}\")\n",
    "\n",
    "import os, pandas as pd\n",
    "\n",
    "csv_path = \"./out_esp/JO PARIS 2024 - lodie Clouvel dcroche la mdaille dargent au pentathlon moderne_with_speaker.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "dialogue_path = \"./out_esp/JO PARIS 2024 - lodie Clouvel dcroche la mdaille dargent au pentathlon moderne_dialogue.txt\"\n",
    "\n",
    "make_dialogue_file(df, dialogue_path, merge_same_speaker=True, max_gap=1.0)\n",
    "\n",
    "with open(dialogue_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for _ in range(10):\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab021fc0-8f07-4780-b3d5-b1e7f0644f41",
   "metadata": {},
   "source": [
    "Problème : transcription pas mééééga précise et diarisation qui crée des trucs pas cohérents comme un interlocuteur 'musique' alors qu'il n'y a que une homme et une femme..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d990b9d-d6cc-4b6c-be67-28336774ecf1",
   "metadata": {},
   "source": [
    "On applique le code à toutes les vidéos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6272098-e7a6-4623-8e3c-3143193310f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_url = \"https://www.youtube.com/playlist?list=PLMvsN4LH5cSFtlag_bbqDc76pyetG_4Ol\"  # ESP\n",
    "\n",
    "ydl_opts = {'quiet': True, 'extract_flat': True}\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    info = ydl.extract_info(playlist_url, download=False)\n",
    "\n",
    "playlist_dict = {entry['title']: entry for entry in info['entries']}\n",
    "\n",
    "# Tu gardes ton slicing\n",
    "entries_to_download = list(playlist_dict.items())[886:]\n",
    "\n",
    "for title, entry in entries_to_download:\n",
    "    # Skip long videos (>25 min)\n",
    "    if entry.get('duration') and entry['duration'] > 1500:\n",
    "        continue\n",
    "\n",
    "    # Safe title for filenames\n",
    "    safe_title = re.sub(r'[^a-zA-Z0-9_\\- ]', '', title)[:100].strip()\n",
    "    \n",
    "    local_audio = f\"/tmp/{safe_title}.webm\"\n",
    "    s3_audio   = f\"s3://lab/PESSD/ESP/audio/{safe_title}.webm\"\n",
    "    s3_txt     = f\"s3://lab/PESSD/ESP/transcripts/{safe_title}.txt\"\n",
    "    s3_csv_seg = f\"s3://lab/PESSD/ESP/segments/{safe_title}_segments.csv\"\n",
    "    s3_csv_spk = f\"s3://lab/PESSD/ESP/segments/{safe_title}_with_speaker.csv\"\n",
    "\n",
    "    # --- 1) Télécharger l'audio ---\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio[ext=webm]',\n",
    "        'outtmpl': local_audio,\n",
    "        'postprocessors': [],\n",
    "        'quiet': True,\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([entry['url']])\n",
    "\n",
    "    # --- 2) Transcription + diarisation sur le fichier local ---\n",
    "    try:\n",
    "        text, df_txt, df_dia = transcribe_and_diarize(local_audio)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur sur {title} : {e}\")\n",
    "        # on supprime le fichier local et on passe à la vidéo suivante\n",
    "        if os.path.exists(local_audio):\n",
    "            os.remove(local_audio)\n",
    "        continue\n",
    "\n",
    "    # --- 3) Sauvegarder les résultats localement ---\n",
    "    local_txt     = f\"/tmp/{safe_title}.txt\"\n",
    "    local_csv_seg = f\"/tmp/{safe_title}_segments.csv\"\n",
    "    local_csv_spk = f\"/tmp/{safe_title}_with_speaker.csv\"\n",
    "\n",
    "    # texte brut\n",
    "    with open(local_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "    # segments diarisation bruts\n",
    "    df_dia.to_csv(local_csv_seg, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # segments Whisper + speaker\n",
    "    df_txt.to_csv(local_csv_spk, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # --- 4) Upload audio + résultats sur S3 ---\n",
    "    fs.put(local_audio, s3_audio)\n",
    "    fs.put(local_txt, s3_txt)\n",
    "    fs.put(local_csv_seg, s3_csv_seg)\n",
    "    fs.put(local_csv_spk, s3_csv_spk)\n",
    "\n",
    "    # --- 5) Nettoyage local ---\n",
    "    os.remove(local_audio)\n",
    "    os.remove(local_txt)\n",
    "    os.remove(local_csv_seg)\n",
    "    os.remove(local_csv_spk)\n",
    "\n",
    "    print(f\"OK : {title}\")\n",
    "\n",
    "print(\"Tâche terminée\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
